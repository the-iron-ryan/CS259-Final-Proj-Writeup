\documentclass{ipgpmaster}
%
% chktex-file 44
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{scalerel,stackengine}
\usepackage{quantikz}
\newcommand{\units}[1]{\ensuremath{\, \mathrm{#1}}}


\begin{document}
\checkyears

\vspace*{5mm}

\setkeys{Gin}{draft=false} % images affich√©es

\selectlanguage{english} 


\def\author{Ryan, Arvind, Payal, and Will}
\def\title{CS 259: Final Project \\ Tensor Networks for Machine Learning}
\def\shorttitle{Two Qubits}
\def\unit{UCLA - MQST}
\def\team{Ryan, Arvind, Payal, and Will}
\def\spe{MQST}
\def\supervisor{Tony Nowatzki}
\def\mydate{\today}

\Entete

\begin{abstract}
For our project we chose to explore the methods of the Google X paper 'Tensor Networks for Machine Learning' by Jack Hidary et al \cite{roberts2019tensornetwork}. Here, the authors explore the use of tensor networks to improve the training performance of classifier neural networks. This is done by using tensor networks to compress the weights of the neural network, which reduces the number of parameters that need to be trained. This reduces the total amount of compute needed to train the network, at the cost of roughly 1\% model accuracy. Modeling the weights of the neural network as a tensor network also allows for the use of parallelism to contract the network (multiply to get the desired result).
\end{abstract}

\newpage
\tableofcontents
\newpage

\section{Methodology}
\subsection{Matrix Product States}
\section{Evaluation}
\section{Conclusion}
\section{Statement of Work}

\nocite{*}
\bibliographystyle{plain} % We choose the "plain" reference style
\bibliography{refs} % Entries are in the refs.bib file

\end{document}